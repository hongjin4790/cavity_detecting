{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmengine\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import utils_ObjectDetection as utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading annotations.json\n",
    "TRAIN_ANNOTATIONS_PATH = \"./train/_annotations.coco.json\"\n",
    "TRAIN_IMAGE_DIRECTIORY = \"./train/\"\n",
    "\n",
    "VAL_ANNOTATIONS_PATH = \"./valid/_annotations.coco.json\"\n",
    "VAL_IMAGE_DIRECTIORY = \"./valid/\"\n",
    "\n",
    "train_coco = COCO(TRAIN_ANNOTATIONS_PATH)\n",
    "valid_coco = COCO(VAL_ANNOTATIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 500,\n",
       " 'image_id': 33,\n",
       " 'category_id': 2,\n",
       " 'bbox': [338, 446, 69.16, 29.77],\n",
       " 'area': 2058.893,\n",
       " 'segmentation': [],\n",
       " 'iscrowd': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the annotation files\n",
    "with open(TRAIN_ANNOTATIONS_PATH) as f:\n",
    "  train_annotations_data = json.load(f)\n",
    "\n",
    "with open(VAL_ANNOTATIONS_PATH) as f:\n",
    "  val_annotations_data = json.load(f)\n",
    "  \n",
    "train_annotations_data['annotations'][500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'name': 'cavity'}, {'id': 2, 'name': 'normal'}]\n"
     ]
    }
   ],
   "source": [
    "category_ids = train_coco.loadCats(train_coco.getCatIds())\n",
    "print(category_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_coco.getCatIds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[335]\n",
      "{'id': 335, 'license': 1, 'file_name': 'healthy_teeth_394_jpg.rf.c4ba13d1347cac2b4de1c9e8eb20e0f6.jpg', 'height': 200, 'width': 252, 'date_captured': '2023-11-07T06:51:21+00:00'}\n"
     ]
    }
   ],
   "source": [
    "im_id = train_coco.getImgIds(imgIds=335)\n",
    "print(im_id)\n",
    "img_info = train_coco.loadImgs(im_id)[0]\n",
    "print(img_info)\n",
    "# print(train_coco.getAnnIds(imgIds=img_info['id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030]\n",
      "[{'id': 4018, 'image_id': 335, 'category_id': 2, 'bbox': [98, 117, 7.89, 10.73], 'area': 84.66, 'segmentation': [], 'iscrowd': 0}, {'id': 4019, 'image_id': 335, 'category_id': 2, 'bbox': [167, 118, 5.91, 9.27], 'area': 54.786, 'segmentation': [], 'iscrowd': 0}, {'id': 4020, 'image_id': 335, 'category_id': 2, 'bbox': [103, 115, 8.25, 15.37], 'area': 126.802, 'segmentation': [], 'iscrowd': 0}, {'id': 4021, 'image_id': 335, 'category_id': 2, 'bbox': [161, 117, 7.98, 13.54], 'area': 108.049, 'segmentation': [], 'iscrowd': 0}, {'id': 4022, 'image_id': 335, 'category_id': 2, 'bbox': [110, 116, 9.43, 17.44], 'area': 164.459, 'segmentation': [], 'iscrowd': 0}, {'id': 4023, 'image_id': 335, 'category_id': 2, 'bbox': [156, 119, 7.25, 14.76], 'area': 107.01, 'segmentation': [], 'iscrowd': 0}, {'id': 4024, 'image_id': 335, 'category_id': 2, 'bbox': [117, 117, 16.02, 20.98], 'area': 336.1, 'segmentation': [], 'iscrowd': 0}, {'id': 4025, 'image_id': 335, 'category_id': 2, 'bbox': [146, 118, 10.91, 18.05], 'area': 196.925, 'segmentation': [], 'iscrowd': 0}, {'id': 4026, 'image_id': 335, 'category_id': 2, 'bbox': [133, 119, 14.44, 19.15], 'area': 276.526, 'segmentation': [], 'iscrowd': 0}, {'id': 4027, 'image_id': 335, 'category_id': 2, 'bbox': [101, 150, 9.6, 8.29], 'area': 79.584, 'segmentation': [], 'iscrowd': 0}, {'id': 4028, 'image_id': 335, 'category_id': 2, 'bbox': [119, 168, 7.52, 7.56], 'area': 56.851, 'segmentation': [], 'iscrowd': 0}, {'id': 4029, 'image_id': 335, 'category_id': 2, 'bbox': [136, 169, 7.12, 6.1], 'area': 43.432, 'segmentation': [], 'iscrowd': 0}, {'id': 4030, 'image_id': 335, 'category_id': 2, 'bbox': [128, 169, 8.98, 8.05], 'area': 72.289, 'segmentation': [], 'iscrowd': 0}]\n"
     ]
    }
   ],
   "source": [
    "ann_ids = train_coco.getAnnIds(imgIds=img_info['id']) #img id, category id를 받아서 해당하는 annotation id 반환\n",
    "anns = train_coco.loadAnns(ann_ids) # annotation id를 받아서 annotation 정보 반환\n",
    "print(ann_ids)\n",
    "print(anns)\n",
    " \n",
    "# 처음에는 이해가 안갔는데 정리하면 한 이미지당 annotation된 충치가 여러개가 있다 \n",
    "# 출력값을 보면 id가 0인 이미지의 annotation은 19개가 있는것이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "class CavityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[index]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "\n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # labels = torch.ones((num_objs,), dtype=torch.int64)  # 충치있으면 1 없으면 2 ### 라벨 문제임\n",
    "        labels = []\n",
    "        for i in range(num_objs):\n",
    "            labels.append(coco_annotation[i]['category_id'])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    " \n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "<__main__.CavityDataset object at 0x0000022D989C2100> <torch.utils.data.dataloader.DataLoader object at 0x0000022D98CD95E0>\n"
     ]
    }
   ],
   "source": [
    "def train_set():\n",
    "    train_data_dir = './train/'\n",
    "    train_coco = './train/_train_anno.json'\n",
    "\n",
    "\n",
    "\n",
    "    cavity_dataset = CavityDataset(root=train_data_dir,\n",
    "                                annotation=train_coco,\n",
    "                                transforms=get_transform())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def collate_fn(batch): # 이거를 사용하면 resize안해도되네? x 상자모양 이상함\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "    train_batch_size = 8\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=cavity_dataset, \n",
    "                                            batch_size=train_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=0,\n",
    "                                            collate_fn=collate_fn\n",
    "                                            )\n",
    "    return cavity_dataset,train_loader\n",
    "cavity_dataset,train_loader = train_set() \n",
    "print(cavity_dataset,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# # DataLoader is iterable over Datasetgftrdec\n",
    "# for imgs, annotations in train_loader:\n",
    "#     imgs = list(img.to(device) for img in imgs)\n",
    "#     annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "#     print(annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_coco = COCO(TRAIN_ANNOTATIONS_PATH)\n",
    "# # anno_id = annotations[0]['image_id'].cpu().numpy()\n",
    "# im_id = train_coco.getImgIds(imgIds=0)\n",
    "# print(im_id)\n",
    "# img_info = train_coco.loadImgs(im_id)[0]['file_name']\n",
    "# print(img_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image, ImageDraw\n",
    "# ann = annotations[0]['boxes'].cpu().numpy()\n",
    "\n",
    "# img = Image.open('./train/' + img_info).convert('RGB')\n",
    "\n",
    "# draw = ImageDraw.Draw(img)\n",
    "# for box in ann:\n",
    "#     draw.rectangle((box[0], box[1], box[2], box[3]), outline=(0,255,0), width = 3)\n",
    "\n",
    "\n",
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "# def get_model_instance_segmentation(num_classes):\n",
    "#     # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "#     model = torchvision.models.detection.fasterrcnn_resnet50_fpn(Weights=False)\n",
    "#     # get number of input features for the classifier\n",
    "#     in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "#     # replace the pre-trained head with a new one\n",
    "#     model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# num_classes = 3  #### Faster R-CNN 사용 시 주의할 점은 background 클래스를 포함한 개수를 num_classes에 명시해야됨 ㅡㅡ \n",
    "# num_epochs = 10\n",
    "# model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# # move model to the right device\n",
    "# model.to(device)\n",
    "# # parameters\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# len_dataloader = len(train_loader)\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     i = 0    \n",
    "#     for imgs, annotations in train_loader:\n",
    "#         i += 1\n",
    "#         imgs = list(img.to(device) for img in imgs)\n",
    "\n",
    "#         annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "\n",
    "#         loss_dict = model(imgs, annotations)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),f'model_{num_epochs}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(Weights=False)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "num_classes = 3\n",
    "num_epochs = 10\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "model.load_state_dict(torch.load(f'model_{num_epochs}.pt'))  \n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_image_from_output(img, annotation):\n",
    "    \n",
    "#     img = img.cpu().permute(1,2,0)\n",
    "    \n",
    "#     fig,ax = plt.subplots(1)\n",
    "#     ax.imshow(img)\n",
    "    \n",
    "#     for idx in range(len(annotation[\"boxes\"])):\n",
    "#         xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx]\n",
    "\n",
    "#         if annotation['labels'][idx] == 1 :  # 충치에 빨간색 네모박스\n",
    "#             rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "        \n",
    "#         elif annotation['labels'][idx] == 2 :  # 정상이면 초록색 네모박스\n",
    "            \n",
    "#             rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n",
    "            \n",
    "\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "<__main__.CavityDataset object at 0x0000022D98640970> <torch.utils.data.dataloader.DataLoader object at 0x0000022D8EB997F0>\n"
     ]
    }
   ],
   "source": [
    "def test_set():\n",
    "    test_data_dir = './valid/'\n",
    "    test_coco = './valid/_annotations.coco.json'\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    def collate_fn(batch): \n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    test_cavity_dataset = CavityDataset(root=test_data_dir,\n",
    "                                        annotation=test_coco,\n",
    "                                        transforms=get_transform())\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_cavity_dataset,\n",
    "                                            batch_size=8,\n",
    "                                            shuffle=True,\n",
    "                                            collate_fn=collate_fn\n",
    "                                            )\n",
    "    return test_cavity_dataset, test_loader\n",
    "\n",
    "test_cavity_dataset, test_loader=test_set()\n",
    "print(test_cavity_dataset, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# test_loader.__len__()\n",
    "len_dataloader = len(test_loader)\n",
    "print(len_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, img, threshold):\n",
    "    model.eval()\n",
    "    preds = model(img)\n",
    "    for id in range(len(preds)) :\n",
    "        idx_list = []\n",
    "\n",
    "        for idx, score in enumerate(preds[id]['scores']) :\n",
    "            if score > threshold : \n",
    "                idx_list.append(idx)\n",
    "\n",
    "        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n",
    "        preds[id]['labels'] = preds[id]['labels'][idx_list]\n",
    "        preds[id]['scores'] = preds[id]['scores'][idx_list]\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for imgs, annotation in test_loader:\n",
    "#         imgs = list(img.to(device) for img in imgs)\n",
    "\n",
    "#         pred = make_prediction(model, imgs, 0.5)\n",
    "#         pred = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in pred] \n",
    "#         print(len(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _idx = 1\n",
    "# print(\"Target : \", annotation[_idx]['labels'])\n",
    "# plot_image_from_output(imgs[_idx], annotation[_idx])\n",
    "# print(\"Prediction : \", pred[_idx]['labels'])\n",
    "# plot_image_from_output(imgs[_idx], pred[_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# labels = []\n",
    "# preds_adj_all = []\n",
    "# annot_all = []\n",
    "\n",
    "# for im, annot in tqdm(test_loader, position = 0, leave = True):\n",
    "#     im = list(img.to(device) for img in im)\n",
    "#     #annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n",
    "\n",
    "#     for t in annot:\n",
    "#         labels += t['labels']\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         preds_adj = make_prediction(model, im, 0.5)\n",
    "#         preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
    "#         preds_adj_all.append(preds_adj)\n",
    "#         annot_all.append(annot)  \n",
    "#     print(len(preds_adj_all), len(annot_all))\n",
    "#     print(preds_adj_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils_ObjectDetection as utils\n",
    "# sample_metrics = []\n",
    "# for batch_i in range(len(preds_adj_all)):\n",
    "#     sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n",
    "\n",
    "# print(sample_metrics)\n",
    "# true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n",
    "# precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n",
    "# mAP = torch.mean(AP)\n",
    "# print(f'mAP : {mAP}')\n",
    "# print(f'AP : {AP}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[===0 epoch===]\n",
      "###########################################################\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "mAP result Epoch: 1, mAP: 0.706395335972964\n",
      "[===1 epoch===]\n",
      "###########################################################\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "mAP result Epoch: 2, mAP: 0.7072254025266562\n",
      "[===2 epoch===]\n",
      "###########################################################\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asiclab\\Desktop\\CNN이론 프로젝트\\cavity.v2-default.coco-mmdetection\\cavityDetection.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmAP result Epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, mAP: \u001b[39m\u001b[39m{\u001b[39;00mmAP\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m mAP\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m main()\n",
      "\u001b[1;32mc:\\Users\\asiclab\\Desktop\\CNN이론 프로젝트\\cavity.v2-default.coco-mmdetection\\cavityDetection.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[===\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m epoch===]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# train_loss, train_mAP = train(model, train_loader, optimizer)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m mAP \u001b[39m=\u001b[39m test(model, test_loader,epoch)\n",
      "\u001b[1;32mc:\\Users\\asiclab\\Desktop\\CNN이론 프로젝트\\cavity.v2-default.coco-mmdetection\\cavityDetection.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m annotations:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     labels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m t[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m preds_adj \u001b[39m=\u001b[39m make_prediction(model, imgs, \u001b[39m0.5\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m preds_adj \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m preds_adj]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m preds_adj_all\u001b[39m.\u001b[39mappend(preds_adj)\n",
      "\u001b[1;32mc:\\Users\\asiclab\\Desktop\\CNN이론 프로젝트\\cavity.v2-default.coco-mmdetection\\cavityDetection.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_prediction\u001b[39m(model, img, threshold):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     preds \u001b[39m=\u001b[39m model(img)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(preds)) :\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asiclab/Desktop/CNN%EC%9D%B4%EB%A1%A0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/cavity.v2-default.coco-mmdetection/cavityDetection.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         idx_list \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\asiclab\\anaconda3\\envs\\cavity\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asiclab\\anaconda3\\envs\\cavity\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m    104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n\u001b[1;32m--> 105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_heads(features, proposals, images\u001b[39m.\u001b[39;49mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    108\u001b[0m losses \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\asiclab\\anaconda3\\envs\\cavity\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\asiclab\\anaconda3\\envs\\cavity\\lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:775\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[1;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[0;32m    773\u001b[0m     losses \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss_classifier\u001b[39m\u001b[39m\"\u001b[39m: loss_classifier, \u001b[39m\"\u001b[39m\u001b[39mloss_box_reg\u001b[39m\u001b[39m\"\u001b[39m: loss_box_reg}\n\u001b[0;32m    774\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 775\u001b[0m     boxes, scores, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpostprocess_detections(class_logits, box_regression, proposals, image_shapes)\n\u001b[0;32m    776\u001b[0m     num_images \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(boxes)\n\u001b[0;32m    777\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_images):\n",
      "File \u001b[1;32mc:\\Users\\asiclab\\anaconda3\\envs\\cavity\\lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:680\u001b[0m, in \u001b[0;36mRoIHeads.postprocess_detections\u001b[1;34m(self, class_logits, box_regression, proposals, image_shapes)\u001b[0m\n\u001b[0;32m    677\u001b[0m num_classes \u001b[39m=\u001b[39m class_logits\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    679\u001b[0m boxes_per_image \u001b[39m=\u001b[39m [boxes_in_image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m boxes_in_image \u001b[39min\u001b[39;00m proposals]\n\u001b[1;32m--> 680\u001b[0m pred_boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbox_coder\u001b[39m.\u001b[39;49mdecode(box_regression, proposals)\n\u001b[0;32m    682\u001b[0m pred_scores \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(class_logits, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    684\u001b[0m pred_boxes_list \u001b[39m=\u001b[39m pred_boxes\u001b[39m.\u001b[39msplit(boxes_per_image, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\asiclab\\anaconda3\\envs\\cavity\\lib\\site-packages\\torchvision\\models\\detection\\_utils.py:178\u001b[0m, in \u001b[0;36mBoxCoder.decode\u001b[1;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39mif\u001b[39;00m box_sum \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    177\u001b[0m     rel_codes \u001b[39m=\u001b[39m rel_codes\u001b[39m.\u001b[39mreshape(box_sum, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 178\u001b[0m pred_boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode_single(rel_codes, concat_boxes)\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m box_sum \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    180\u001b[0m     pred_boxes \u001b[39m=\u001b[39m pred_boxes\u001b[39m.\u001b[39mreshape(box_sum, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\asiclab\\anaconda3\\envs\\cavity\\lib\\site-packages\\torchvision\\models\\detection\\_utils.py:216\u001b[0m, in \u001b[0;36mBoxCoder.decode_single\u001b[1;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[0;32m    213\u001b[0m pred_h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(dh) \u001b[39m*\u001b[39m heights[:, \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m    215\u001b[0m \u001b[39m# Distance from center to box's corner.\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m c_to_c_h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(\u001b[39m0.5\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mpred_ctr_y\u001b[39m.\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mpred_h\u001b[39m.\u001b[39;49mdevice) \u001b[39m*\u001b[39m pred_h\n\u001b[0;32m    217\u001b[0m c_to_c_w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.5\u001b[39m, dtype\u001b[39m=\u001b[39mpred_ctr_x\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mpred_w\u001b[39m.\u001b[39mdevice) \u001b[39m*\u001b[39m pred_w\n\u001b[0;32m    219\u001b[0m pred_boxes1 \u001b[39m=\u001b[39m pred_ctr_x \u001b[39m-\u001b[39m c_to_c_w\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 이거 돌려야됨\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(Weights=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    num_classes = 3  #### Faster R-CNN 사용 시 주의할 점은 background 클래스를 포함한 개수를 num_classes에 명시해야됨 ㅡㅡ \n",
    "    num_epochs = 10\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f'model_{num_epochs}.pt'))  \n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "    # parameters\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    train_dataset, train_loader = train_set()\n",
    "    test_dataset, test_loader = test_set()\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('[==={0} epoch===]'.format(epoch))\n",
    "        # train_loss, train_mAP = train(model, train_loader, optimizer)\n",
    "        mAP = test(model, test_loader,epoch)\n",
    "    \n",
    "    \n",
    "\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    len_dataloader = train_loader.__len__()\n",
    "    i = 0    \n",
    "    for imgs, annotations in train_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        #print(f'TRAIN Iteration: {i}/{len_dataloader}, Loss: {losses}, mAP: {mAP}')\n",
    "        print(f'TRAIN Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "    #return train_loss, train_mAP\n",
    "\n",
    "# def mAP_result(model, epoch, loader):\n",
    "\n",
    "       \n",
    "\n",
    "#         for t in annot:\n",
    "#             labels += t['labels']\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             preds_adj = make_prediction(model, im, 0.5)\n",
    "#             preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
    "#             preds_adj_all.append(preds_adj)\n",
    "#             annot_all.append(annot)  \n",
    "\n",
    "    \n",
    "#     for batch_i in range(len(preds_adj_all)):\n",
    "#         sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n",
    "\n",
    "#     true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n",
    "#     precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n",
    "#     mAP = torch.mean(AP)\n",
    "#     print(f'mAP result Epoch: {epoch+1}, mAP: {mAP}')\n",
    "#     return mAP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_loader, epoch):\n",
    "    print(\"###########################################################\")\n",
    "    len_dataloader = test_loader.__len__()\n",
    "    labels = []\n",
    "    preds_adj_all = []\n",
    "    annot_all = []\n",
    "    sample_metrics = []\n",
    "    total_losses = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i = 0 \n",
    "        for imgs, annotations in test_loader:\n",
    "            model.train()\n",
    "            print(i)\n",
    "            i += 1\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            annota = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "            loss_dict = model(imgs, annota)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_losses += losses\n",
    "            \n",
    "            for t in annotations:\n",
    "                labels += t['labels']\n",
    "\n",
    "            preds_adj = make_prediction(model, imgs, 0.5)\n",
    "            preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
    "            preds_adj_all.append(preds_adj)\n",
    "            annot_all.append(annotations)  \n",
    "\n",
    "        for batch_i in range(len(preds_adj_all)):\n",
    "            sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n",
    "\n",
    "        true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n",
    "        precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n",
    "        mAP = torch.mean(AP)\n",
    "        print(f'mAP result Epoch: {epoch+1}, mAP: {mAP}')\n",
    "        return mAP\n",
    "        \n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
