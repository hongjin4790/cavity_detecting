{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmengine\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import utils_ObjectDetection as utils\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import utils_ObjectDetection as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading annotations.json\n",
    "TRAIN_ANNOTATIONS_PATH = \"./train/_annotations.coco.json\"\n",
    "TRAIN_IMAGE_DIRECTIORY = \"./train/\"\n",
    "\n",
    "VAL_ANNOTATIONS_PATH = \"./valid/_annotations.coco.json\"\n",
    "VAL_IMAGE_DIRECTIORY = \"./valid/\"\n",
    "\n",
    "train_coco = COCO(TRAIN_ANNOTATIONS_PATH)\n",
    "valid_coco = COCO(VAL_ANNOTATIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CavityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[index]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "\n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # labels = torch.ones((num_objs,), dtype=torch.int64)  # 충치있으면 1 없으면 2 ### 라벨 문제임\n",
    "        labels = []\n",
    "        for i in range(num_objs):\n",
    "            labels.append(coco_annotation[i]['category_id'])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    " \n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "<__main__.CavityDataset object at 0x000002442A1E42B0> <torch.utils.data.dataloader.DataLoader object at 0x000002441ECC7580>\n"
     ]
    }
   ],
   "source": [
    "def train_set():\n",
    "    train_data_dir = './train/'\n",
    "    train_coco = './train/_train_anno.json'\n",
    "\n",
    "\n",
    "\n",
    "    cavity_dataset = CavityDataset(root=train_data_dir,\n",
    "                                annotation=train_coco,\n",
    "                                transforms=get_transform())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def collate_fn(batch): # 이거를 사용하면 resize안해도되네? x 상자모양 이상함\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "    train_batch_size = 16\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=cavity_dataset, \n",
    "                                            batch_size=train_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=0,\n",
    "                                            collate_fn=collate_fn\n",
    "                                            )\n",
    "    return cavity_dataset,train_loader\n",
    "cavity_dataset,train_loader = train_set() \n",
    "print(cavity_dataset,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),f'model_{num_epochs}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "<__main__.CavityDataset object at 0x000002442A1E4910> <torch.utils.data.dataloader.DataLoader object at 0x000002441EE21190>\n"
     ]
    }
   ],
   "source": [
    "def test_set():\n",
    "    test_data_dir = './valid/'\n",
    "    test_coco = './valid/_annotations.coco.json'\n",
    "    def collate_fn(batch): \n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    test_cavity_dataset = CavityDataset(root=test_data_dir,\n",
    "                                        annotation=test_coco,\n",
    "                                        transforms=get_transform())\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_cavity_dataset,\n",
    "                                            batch_size=16,\n",
    "                                            shuffle=True,\n",
    "                                            collate_fn=collate_fn\n",
    "                                            )\n",
    "    return test_cavity_dataset, test_loader\n",
    "\n",
    "test_cavity_dataset, test_loader=test_set()\n",
    "print(test_cavity_dataset, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# test_loader.__len__()\n",
    "len_dataloader = len(test_loader)\n",
    "print(len_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, img, threshold):\n",
    "    model.eval()\n",
    "    preds = model(img)\n",
    "    for id in range(len(preds)) :\n",
    "        idx_list = []\n",
    "\n",
    "        for idx, score in enumerate(preds[id]['scores']) :\n",
    "            if score > threshold : \n",
    "                idx_list.append(idx)\n",
    "\n",
    "        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n",
    "        preds[id]['labels'] = preds[id]['labels'][idx_list]\n",
    "        preds[id]['scores'] = preds[id]['scores'][idx_list]\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asiclab\\anaconda3\\envs\\cavity\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\asiclab\\anaconda3\\envs\\cavity\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to C:\\Users\\asiclab/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ee6770a6a34e1e81e52dfbf529e7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/160M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "def list2text(data_dic):\n",
    "    with open('dictionary.txt', 'w') as file:\n",
    "        for key, value in data_dic.items():\n",
    "            file.write(key + ': ' + str(value) + '\\n')\n",
    "            \n",
    "def main():\n",
    "    num_classes = 3  #### Faster R-CNN 사용 시 주의할 점은 background 클래스를 포함한 개수를 num_classes에 명시해야됨 ㅡㅡ \n",
    "    num_epochs = 50\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "    \n",
    "    #model.load_state_dict(torch.load(f'model_{num_epochs}.pt'))  \n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "    # parameters\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    train_dataset, train_loader = train_set()\n",
    "    test_dataset, test_loader = test_set()\n",
    "    \n",
    "    data_dic = {\n",
    "        'train_loss':[],\n",
    "        'test_loss':[],\n",
    "        'train_map':[],\n",
    "        'test_map':[]\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_mAP = train(model, train_loader, optimizer)\n",
    "        test_loss, test_mAP = test(model, test_loader)\n",
    "\n",
    "        data_dic['train_loss'].append(train_loss)\n",
    "        data_dic['test_loss'].append(test_loss)\n",
    "        data_dic['train_map'].append(train_mAP)\n",
    "        data_dic['test_map'].append(test_mAP)\n",
    "\n",
    "        print('[================================================={0} epoch=================================================]'.format(epoch+1))\n",
    "        print('++++++++++RESULT+++++++++')\n",
    "        print('train l: ',train_loss.item(), 'train m: ',train_mAP.item())\n",
    "        print('test l: ',test_loss.item(), 'test m:', test_mAP.item())\n",
    "        print('[===========================================================================================================]')\n",
    "    \n",
    "    \n",
    "    list2text(data_dic)\n",
    "\n",
    "    torch.save(model.state_dict(),f'model_{num_epochs}.pt')\n",
    "    \n",
    "def train(model, train_loader, optimizer):\n",
    "    len_dataloader = train_loader.__len__()\n",
    "    i = 0    \n",
    "    labels = []\n",
    "    preds_adj_all = []\n",
    "    annot_all = []\n",
    "    sample_metrics = []\n",
    "    train_total_losses = 0\n",
    "\n",
    "    for imgs, annotations in train_loader:\n",
    "        model.train()\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        train_total_losses += losses\n",
    "        \n",
    "        for t in annotations:\n",
    "            labels += t['labels']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds_adj = make_prediction(model, imgs, 0.5)\n",
    "            preds_adj = [{k: v.to(device) for k, v in t.items()} for t in preds_adj]\n",
    "            preds_adj_all.append(preds_adj)\n",
    "            annot_all.append(annotations)\n",
    "            \n",
    "        print(f'TRAIN Iteration: {i}/{len_dataloader}, Loss: {losses}')    \n",
    "        \n",
    "    for batch_i in range(len(preds_adj_all)):\n",
    "        sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n",
    "        \n",
    "    true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n",
    "    precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n",
    "\n",
    "    train_total_losses /= len_dataloader\n",
    "    train_mAP = torch.mean(AP)\n",
    "    return train_total_losses, train_mAP\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    print(\"###########################################################\")\n",
    "    len_dataloader = test_loader.__len__()\n",
    "    labels = []\n",
    "    preds_adj_all = []\n",
    "    annot_all = []\n",
    "    sample_metrics = []\n",
    "    test_total_losses = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i = 0 \n",
    "        for imgs, annotations in test_loader:\n",
    "            model.train()\n",
    "            i += 1\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            annota = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "            loss_dict = model(imgs, annota)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            test_total_losses += losses\n",
    "            \n",
    "            for t in annota:\n",
    "                labels += t['labels']\n",
    "\n",
    "            preds_adj = make_prediction(model, imgs, 0.5)\n",
    "            preds_adj = [{k: v.to(device) for k, v in t.items()} for t in preds_adj]\n",
    "            preds_adj_all.append(preds_adj)\n",
    "            annot_all.append(annota)  \n",
    "            \n",
    "            print(f' Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "            \n",
    "    for batch_i in range(len(preds_adj_all)):\n",
    "        sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n",
    "\n",
    "    true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n",
    "    precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n",
    "    \n",
    "    test_total_losses /= len_dataloader\n",
    "    test_mAP = torch.mean(AP)\n",
    "    return test_total_losses, test_mAP\n",
    "    \n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
